{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBESQc0mQdf5",
        "outputId": "ad7b5e81-28d1-4359-9d86-873b88c00e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations==1.2.1 in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (4.6.0.66)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (0.18.3)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (1.7.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (4.1.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (1.0.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2021.11.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.1) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.3\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install albumentations==1.2.1\n",
        "!pip install torchmetrics\n",
        "import os\n",
        "from collections import Counter\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim as optim\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import torchmetrics\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "import gc\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Config"
      ],
      "metadata": {
        "id": "2kcOhYzuh01v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET = 'PASCAL_VOC'\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# seed_everything()  # If you want deterministic behavior\n",
        "NUM_WORKERS = 2\n",
        "BATCH_SIZE = 16\n",
        "IMAGE_SIZE = 416\n",
        "NUM_CLASSES = 2\n",
        "LEARNING_RATE = 3e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "NUM_EPOCHS = 43\n",
        "CONF_THRESHOLD = 0.5\n",
        "MAP_IOU_THRESH = 0.5\n",
        "NMS_IOU_THRESH = 0.45\n",
        "S = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8]\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = True\n",
        "SAVE_MODEL = True\n",
        "CHECKPOINT_FILE = \"checkpoint.pth.tar\"\n",
        "ANCHORS = [\n",
        "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
        "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
        "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
        "] \n",
        "# IMG_DIR = DATASET + \"/images/\"\n",
        "# LABEL_DIR = DATASET + \"/labels/\""
      ],
      "metadata": {
        "id": "Ep0JhNs6h2Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "dRPCbDVSiNv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_width_height(boxes1, boxes2):\n",
        "    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
        "        boxes1[..., 1], boxes2[..., 1]\n",
        "    )\n",
        "    union = (\n",
        "        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
        "    )\n",
        "    return intersection / union\n",
        "\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"midpoint\"):\n",
        "\n",
        "    assert type(bboxes) == list\n",
        "\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "    bboxes_after_nms = []\n",
        "\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "            or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format,\n",
        "            )\n",
        "            < iou_threshold\n",
        "        ]\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms\n",
        "    \n",
        "def mean_average_precision(\n",
        "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=2\n",
        "):\n",
        "\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        # Go through all predictions and targets,\n",
        "        # and only add the ones that belong to the\n",
        "        # current class c\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        # find the amount of bboxes for each training example\n",
        "        # Counter here finds how many ground truth bboxes we get\n",
        "        # for each training example, so let's say img 0 has 3,\n",
        "        # img 1 has 5 then we will obtain a dictionary with:\n",
        "        # amount_bboxes = {0:3, 1:5}\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        # We then go through each key, val in this dictionary\n",
        "        # and convert to the following (w.r.t same example):\n",
        "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        # sort by box probabilities which is index 2\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "\n",
        "        # If none exists for this class then we can safely skip\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            # Only take out the ground_truths that have the same\n",
        "            # training idx as detection\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                # only detect ground truth detection once\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    # true positive and add this bounding box to seen\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "\n",
        "            # if IOU is lower then the detection is a false positive\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        # torch.trapz for numerical integration\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "def cells_to_bboxes(predictions, anchors, S, is_preds=True):\n",
        "    BATCH_SIZE = predictions.shape[0]\n",
        "    num_anchors = len(anchors)\n",
        "    box_predictions = predictions[..., 1:5]\n",
        "    if is_preds:\n",
        "        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
        "        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
        "        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n",
        "        scores = torch.sigmoid(predictions[..., 0:1])\n",
        "        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
        "    else:\n",
        "        scores = predictions[..., 0:1]\n",
        "        best_class = predictions[..., 5:6]\n",
        "\n",
        "    cell_indices = (\n",
        "        torch.arange(S)\n",
        "        .repeat(predictions.shape[0], 3, S, 1)\n",
        "        .unsqueeze(-1)\n",
        "        .to(predictions.device)\n",
        "    )\n",
        "    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n",
        "    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
        "    w_h = 1 / S * box_predictions[..., 2:4]\n",
        "    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n",
        "    return converted_bboxes.tolist()\n",
        "\n",
        "def get_evaluation_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    iou_threshold,\n",
        "    anchors,\n",
        "    threshold,\n",
        "    box_format=\"midpoint\",\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    # make sure model is in eval before get bboxes\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "    for batch_idx, (x, labels) in enumerate(tqdm(loader)):\n",
        "        x = x.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        bboxes = [[] for _ in range(batch_size)]\n",
        "        for i in range(3):\n",
        "            S = predictions[i].shape[2]\n",
        "            anchor = torch.tensor([*anchors[i]]).to(device) * S\n",
        "            boxes_scale_i = cells_to_bboxes(\n",
        "                predictions[i], anchor, S=S, is_preds=True\n",
        "            )\n",
        "            for idx, (box) in enumerate(boxes_scale_i):\n",
        "                bboxes[idx] += box\n",
        "\n",
        "        # we just want one bbox for each label, not one for each scale\n",
        "        true_bboxes = cells_to_bboxes(\n",
        "            labels[2], anchor, S=S, is_preds=False\n",
        "        )\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "                bboxes[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "                threshold=threshold,\n",
        "                box_format=box_format,\n",
        "            )\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_bboxes[idx]:\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "def plot_image(image, boxes):\n",
        "    cmap = plt.get_cmap(\"tab20b\")\n",
        "    class_labels = [\"fine\", \"not fine\"]\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n",
        "    im = np.array(image)\n",
        "    height, width, _ = im.shape\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots(1)\n",
        "    # Display the image\n",
        "    ax.imshow(im)\n",
        "\n",
        "    # box[0] is x midpoint, box[2] is width\n",
        "    # box[1] is y midpoint, box[3] is height\n",
        "\n",
        "    # Create a Rectangle patch\n",
        "    for box in boxes:\n",
        "        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
        "        class_pred = box[0]\n",
        "        box = box[2:]\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=2,\n",
        "            edgecolor=colors[int(class_pred)],\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(\n",
        "            upper_left_x * width,\n",
        "            upper_left_y * height,\n",
        "            s=class_labels[int(class_pred)],\n",
        "            color=\"white\",\n",
        "            verticalalignment=\"top\",\n",
        "            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
        "        )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def check_class_accuracy(model, loader, threshold):\n",
        "    model.eval()\n",
        "    tot_class_preds, correct_class = 0, 0\n",
        "    tot_noobj, correct_noobj = 0, 0\n",
        "    tot_obj, correct_obj = 0, 0\n",
        "\n",
        "    for idx, (x, y) in enumerate(tqdm(loader)):\n",
        "        x = x.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            out = model(x)\n",
        "\n",
        "        for i in range(3):\n",
        "            y[i] = y[i].to(DEVICE)\n",
        "            obj = y[i][..., 0] == 1 # in paper this is Iobj_i\n",
        "            noobj = y[i][..., 0] == 0  # in paper this is Iobj_i\n",
        "\n",
        "            correct_class += torch.sum(\n",
        "                torch.argmax(out[i][..., 5:][obj], dim=-1) == y[i][..., 5][obj]\n",
        "            )\n",
        "            tot_class_preds += torch.sum(obj)\n",
        "\n",
        "            obj_preds = torch.sigmoid(out[i][..., 0]) > threshold\n",
        "            correct_obj += torch.sum(obj_preds[obj] == y[i][..., 0][obj])\n",
        "            tot_obj += torch.sum(obj)\n",
        "            correct_noobj += torch.sum(obj_preds[noobj] == y[i][..., 0][noobj])\n",
        "            tot_noobj += torch.sum(noobj)\n",
        "\n",
        "    print(f\"Class accuracy is: {(correct_class/(tot_class_preds+1e-16))*100:2f}%\")\n",
        "    print(f\"No obj accuracy is: {(correct_noobj/(tot_noobj+1e-16))*100:2f}%\")\n",
        "    print(f\"Obj accuracy is: {(correct_obj/(tot_obj+1e-16))*100:2f}%\")\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def seed_everything(seed=0):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "Wbl9fhAfiO71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "9ZQ214FSVcCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = [\n",
        "    (32, 3, 1),\n",
        "    (64, 3, 2),\n",
        "    [\"B\", 1],\n",
        "    (128, 3, 2),\n",
        "    [\"B\", 2],\n",
        "    (256, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (512, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (1024, 3, 2),\n",
        "    [\"B\", 4],  # To this point is Darknet-53\n",
        "    (512, 1, 1),\n",
        "    (1024, 3, 1),\n",
        "    \"S\",\n",
        "    (256, 1, 1),\n",
        "    \"U\",\n",
        "    (256, 1, 1),\n",
        "    (512, 3, 1),\n",
        "    \"S\",\n",
        "    (128, 1, 1),\n",
        "    \"U\",\n",
        "    (128, 1, 1),\n",
        "    (256, 3, 1),\n",
        "    \"S\",\n",
        "]\n",
        "\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.1)\n",
        "        self.use_bn_act = bn_act\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_bn_act:\n",
        "            return self.leaky(self.bn(self.conv(x)))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for repeat in range(num_repeats):\n",
        "            self.layers += [\n",
        "                nn.Sequential(\n",
        "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
        "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "        self.use_residual = use_residual\n",
        "        self.num_repeats = num_repeats\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            if self.use_residual:\n",
        "                x = x + layer(x)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ScalePrediction(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.pred = nn.Sequential(\n",
        "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
        "            CNNBlock(\n",
        "                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n",
        "            ),\n",
        "        )\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (\n",
        "            self.pred(x)\n",
        "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
        "            .permute(0, 1, 3, 4, 2)\n",
        "        )\n",
        "\n",
        "\n",
        "class YOLOv3(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.in_channels = in_channels\n",
        "        self.layers = self._create_conv_layers()\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []  # for each scale\n",
        "        route_connections = []\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, ScalePrediction):\n",
        "                outputs.append(layer(x))\n",
        "                continue\n",
        "\n",
        "            x = layer(x)\n",
        "\n",
        "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
        "                route_connections.append(x)\n",
        "\n",
        "            elif isinstance(layer, nn.Upsample):\n",
        "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
        "                route_connections.pop()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _create_conv_layers(self):\n",
        "        layers = nn.ModuleList()\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for module in config:\n",
        "            if isinstance(module, tuple):\n",
        "                out_channels, kernel_size, stride = module\n",
        "                layers.append(\n",
        "                    CNNBlock(\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        kernel_size=kernel_size,\n",
        "                        stride=stride,\n",
        "                        padding=1 if kernel_size == 3 else 0,\n",
        "                    )\n",
        "                )\n",
        "                in_channels = out_channels\n",
        "\n",
        "            elif isinstance(module, list):\n",
        "                num_repeats = module[1]\n",
        "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
        "\n",
        "            elif isinstance(module, str):\n",
        "                if module == \"S\":\n",
        "                    layers += [\n",
        "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
        "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
        "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
        "                    ]\n",
        "                    in_channels = in_channels // 2\n",
        "\n",
        "                elif module == \"U\":\n",
        "                    layers.append(nn.Upsample(scale_factor=2),)\n",
        "                    in_channels = in_channels * 3\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n",
        "\n",
        "# num_classes = 2\n",
        "# IMAGE_SIZE = 416\n",
        "# model = YOLOv3(num_classes=num_classes)\n",
        "# x = torch.randn((2, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
        "# out = model(x)\n",
        "# assert model(x)[0].shape == (2, 3, IMAGE_SIZE//32, IMAGE_SIZE//32, num_classes + 5)\n",
        "# assert model(x)[1].shape == (2, 3, IMAGE_SIZE//16, IMAGE_SIZE//16, num_classes + 5)\n",
        "# assert model(x)[2].shape == (2, 3, IMAGE_SIZE//8, IMAGE_SIZE//8, num_classes + 5)\n",
        "# print(\"Success!\")"
      ],
      "metadata": {
        "id": "nUHq_mlHVdUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "go0k29z-diGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zipFile = zipfile.ZipFile('/content/drive/MyDrive/Datasets/detection_db_new.zip', 'r')\n",
        "zipFile.extractall('dataset')\n",
        "zipFile.close()"
      ],
      "metadata": {
        "id": "xxhPNgZyXKsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 416\n",
        "train_transforms1 = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
        "        A.Blur(p=0.3),\n",
        "        A.CLAHE(p=0.4),\n",
        "        A.ChannelShuffle(p=0.5),\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
        ")\n",
        "\n",
        "train_transforms2 = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
        "        A.Posterize(p=0.1),\n",
        "        A.ToGray(p=0.1),\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
        ")\n",
        "test_transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
        ")"
      ],
      "metadata": {
        "id": "1sd394pzl-kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLODataset(Dataset):\n",
        "  def __init__(\n",
        "    self,\n",
        "    img_dir,\n",
        "    label_dir,\n",
        "    anchors,\n",
        "    image_size=416,\n",
        "    S=[13, 26, 52],\n",
        "    C=2,\n",
        "    transform=None,\n",
        "  ):\n",
        "\n",
        "    self.img_dir = img_dir\n",
        "    self.label_dir = label_dir\n",
        "    all_images = os.listdir(self.img_dir)\n",
        "    all_labels = os.listdir(self.label_dir)\n",
        "    all_images.sort()\n",
        "    all_labels.sort()\n",
        "\n",
        "    all_images.pop(0)\n",
        "    all_labels.pop(0)\n",
        "\n",
        "    for i in range(len(all_images)):\n",
        "      all_images[i] = all_images[i].replace('.jpeg', '')\n",
        "\n",
        "    for i in range(len(all_labels)):\n",
        "      all_labels[i] = all_labels[i].replace('.txt', '')\n",
        "\n",
        "    self.images = []\n",
        "    for image in all_images:\n",
        "      if image in all_labels:\n",
        "        self.images.append(image)\n",
        "\n",
        "\n",
        "    self.image_size = image_size\n",
        "    self.transform = transform\n",
        "    self.S = S\n",
        "    self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
        "    self.num_anchors = self.anchors.shape[0]\n",
        "    self.num_anchors_per_scale = self.num_anchors // 3\n",
        "    self.C = C\n",
        "    self.ignore_iou_thresh = 0.5\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    annotation = self.images[index]+\".txt\"\n",
        "    img_name = self.images[index]+\".jpeg\"\n",
        "    label_path = os.path.join(self.label_dir, annotation)\n",
        "    img_path = os.path.join(self.img_dir, img_name)\n",
        "    bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
        "    bboxes = np.clip(bboxes,0,1)\n",
        "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "    if self.transform:\n",
        "      augmentations = self.transform(image=image, bboxes=bboxes)\n",
        "      image = augmentations[\"image\"]\n",
        "      bboxes = augmentations[\"bboxes\"]\n",
        "\n",
        "\n",
        "\n",
        "    # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
        "    targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
        "    for box in bboxes:\n",
        "      iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors)\n",
        "      anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
        "      x, y, width, height, class_label = box\n",
        "      has_anchor = [False] * 3  # each scale should have one anchor\n",
        "      for anchor_idx in anchor_indices:\n",
        "        scale_idx = anchor_idx // self.num_anchors_per_scale\n",
        "        anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
        "        S = self.S[scale_idx]\n",
        "        i, j = int(S * y), int(S * x)  # which cell\n",
        "        anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
        "        if not anchor_taken and not has_anchor[scale_idx]:\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
        "          x_cell, y_cell = S * x - j, S * y - i  # both between [0,1]\n",
        "          width_cell, height_cell = (\n",
        "              width * S,\n",
        "              height * S,\n",
        "          )  # can be greater than 1 since it's relative to cell\n",
        "          box_coordinates = torch.tensor(\n",
        "              [x_cell, y_cell, width_cell, height_cell]\n",
        "          )\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
        "          has_anchor[scale_idx] = True\n",
        "\n",
        "        elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
        "\n",
        "    return image, tuple(targets)"
      ],
      "metadata": {
        "id": "Nn3BVvIkdvgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset1 = YOLODataset(\"/content/dataset/detection_db/images/train\",\n",
        "                            \"/content/dataset/detection_db/labels/train\",\n",
        "                            anchors=ANCHORS,\n",
        "                            S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "                            transform=train_transforms1)\n",
        "\n",
        "train_dataset2 = YOLODataset(\"/content/dataset/detection_db/images/train\",\n",
        "                            \"/content/dataset/detection_db/labels/train\",\n",
        "                            anchors=ANCHORS,\n",
        "                            S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "                            transform=train_transforms2)\n",
        "\n",
        "val_dataset = YOLODataset(\"/content/dataset/detection_db/images/val\",\n",
        "                            \"/content/dataset/detection_db/labels/val\",\n",
        "                            anchors=ANCHORS,\n",
        "                            S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "                            transform=test_transforms)\n",
        "\n",
        "test_dataset = YOLODataset(\"/content/dataset/detection_db/images/val\",\n",
        "                            \"/content/dataset/detection_db/labels/val\",\n",
        "                            anchors=ANCHORS,\n",
        "                            S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "                            transform=test_transforms)\n",
        "\n",
        "train_dataset = torch.utils.data.ConcatDataset([train_dataset1, train_dataset2])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "val_loader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "ChX3iLUaqFpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test dataset class"
      ],
      "metadata": {
        "id": "R3v90KG5IiGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    anchors = ANCHORS\n",
        "\n",
        "\n",
        "    dataset = YOLODataset(\n",
        "        \"/content/dataset/detection_db/images/train\",\n",
        "        \"/content/dataset/detection_db/labels/train\",\n",
        "        S=[13, 26, 52],\n",
        "        anchors=anchors,\n",
        "        transform=train_transforms,\n",
        "    )\n",
        "    S = [13, 26, 52]\n",
        "    scaled_anchors = torch.tensor(anchors) / (\n",
        "        1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "    )\n",
        "    loader = DataLoader(dataset=dataset, batch_size=4, shuffle=True)\n",
        "    for x, y in loader:\n",
        "        boxes = []\n",
        "\n",
        "        for i in range(y[0].shape[1]):\n",
        "            anchor = scaled_anchors[i]\n",
        "            print(anchor.shape)\n",
        "            print(y[i].shape)\n",
        "            boxes += cells_to_bboxes(\n",
        "                y[i], is_preds=False, S=y[i].shape[2], anchors=anchor\n",
        "            )[0]\n",
        "        boxes = non_max_suppression(boxes, iou_threshold=1, threshold=0.7, box_format=\"midpoint\")\n",
        "        print(boxes)\n",
        "        # plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), boxes)\n",
        "    print(\"Success!\")\n",
        "\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "illYL_9tlsir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss function"
      ],
      "metadata": {
        "id": "tNW4ZXH-p0OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.entropy = nn.CrossEntropyLoss()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Constants signifying how much to pay for each respective part of the loss\n",
        "        self.lambda_class = 1\n",
        "        self.lambda_noobj = 1\n",
        "        self.lambda_obj = 1\n",
        "        self.lambda_box = 1\n",
        "\n",
        "    def forward(self, predictions, target, anchors):\n",
        "        # Check where obj and noobj (we ignore if target == -1)\n",
        "        obj = target[..., 0] == 1  # in paper this is Iobj_i\n",
        "        noobj = target[..., 0] == 0  # in paper this is Inoobj_i\n",
        "\n",
        "        # ======================= #\n",
        "        #   FOR NO OBJECT LOSS    #\n",
        "        # ======================= #\n",
        "\n",
        "        no_object_loss = self.bce(\n",
        "            (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]),\n",
        "        )\n",
        "\n",
        "        # ==================== #\n",
        "        #   FOR OBJECT LOSS    #\n",
        "        # ==================== #\n",
        "\n",
        "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
        "        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
        "        ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
        "        object_loss = self.mse(self.sigmoid(predictions[..., 0:1][obj]), ious * target[..., 0:1][obj])\n",
        "\n",
        "        # ======================== #\n",
        "        #   FOR BOX COORDINATES    #\n",
        "        # ======================== #\n",
        "\n",
        "        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])  # x,y coordinates\n",
        "        target[..., 3:5] = torch.log(\n",
        "            (1e-16 + target[..., 3:5] / anchors)\n",
        "        )  # width, height coordinates\n",
        "        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
        "\n",
        "        # ================== #\n",
        "        #   FOR CLASS LOSS   #\n",
        "        # ================== #\n",
        "\n",
        "        class_loss = self.entropy(\n",
        "            (predictions[..., 5:][obj]), (target[..., 5][obj].long()),\n",
        "        )\n",
        "\n",
        "        #print(\"__________________________________\")\n",
        "        #print(self.lambda_box * box_loss)\n",
        "        #print(self.lambda_obj * object_loss)\n",
        "        #print(self.lambda_noobj * no_object_loss)\n",
        "        #print(self.lambda_class * class_loss)\n",
        "        #print(\"\\n\")\n",
        "\n",
        "        return (\n",
        "            self.lambda_box * box_loss\n",
        "            + self.lambda_obj * object_loss\n",
        "            + self.lambda_noobj * no_object_loss\n",
        "            + self.lambda_class * class_loss\n",
        "        )"
      ],
      "metadata": {
        "id": "tJU9yAbqp2PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "JKZSwBHsrogH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    losses = []\n",
        "    for batch_idx, (x, y) in enumerate(loop):\n",
        "        scaled_anchors = scaled_anchors.to(DEVICE)\n",
        "        x = x.to(DEVICE)\n",
        "        y0, y1, y2 = (\n",
        "            y[0].to(DEVICE),\n",
        "            y[1].to(DEVICE),\n",
        "            y[2].to(DEVICE),\n",
        "        )\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(x)\n",
        "            loss = (\n",
        "                loss_fn(out[0], y0, scaled_anchors[0])\n",
        "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "            )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # update progress bar\n",
        "        mean_loss = sum(losses) / len(losses)\n",
        "        loop.set_postfix(loss=mean_loss)"
      ],
      "metadata": {
        "id": "cuc9X1ty17zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "MP0yOolb-Zbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed_everything()\n",
        "model = YOLOv3(num_classes=2).to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "loss_fn = YoloLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "S = [13, 26, 52]\n",
        "scaled_anchors = torch.tensor(ANCHORS) / (\n",
        "        1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        ")\n",
        "\n",
        "\n",
        "writer = SummaryWriter(\"/content/boards/board\")\n",
        "step = 0\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  model.train()\n",
        "  #plot_couple_examples(model, test_loader, 0.6, 0.5, scaled_anchors)\n",
        "  train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
        "\n",
        "\n",
        "  #check_class_accuracy(model, test_loader, threshold=CONF_THRESHOLD)\n",
        "  model.eval()\n",
        "  pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "        test_loader,\n",
        "        model,\n",
        "        iou_threshold=NMS_IOU_THRESH,\n",
        "        anchors=ANCHORS,\n",
        "        threshold=CONF_THRESHOLD,\n",
        "    )\n",
        "  mapval = mean_average_precision(\n",
        "          pred_boxes,\n",
        "          true_boxes,\n",
        "          iou_threshold=MAP_IOU_THRESH,\n",
        "          box_format=\"midpoint\",\n",
        "          num_classes=NUM_CLASSES,\n",
        "      )\n",
        "  print(\"mAP = \", mapval.item())\n",
        "  writer.add_scalar(\"mAP\", mapval.item(), global_step=step)\n",
        "  step += 1\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "PjVd3E411pVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test model"
      ],
      "metadata": {
        "id": "zuejorukevvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = torch.load(\"/content/drive/MyDrive/Models/detection_YOLO_loss=0,245\")\n",
        "model.eval()\n",
        "S = [13, 26, 52]\n",
        "scaled_anchors = torch.tensor(ANCHORS) / (\n",
        "        1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        ")\n",
        "for x, y in train_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        boxes = []\n",
        "        y = model(x)\n",
        "\n",
        "        for i in range(y[0].shape[1]):\n",
        "            anchor = scaled_anchors[i]\n",
        "            anchor = anchor.to(DEVICE)\n",
        "            print(anchor.shape)\n",
        "            print(y[i].shape)\n",
        "            boxes += cells_to_bboxes(\n",
        "                y[i], is_preds=True, S=y[i].shape[2], anchors=anchor\n",
        "            )[0]\n",
        "        # print(boxes)\n",
        "        boxes = non_max_suppression(boxes, iou_threshold=0.4, threshold=0.88, box_format=\"midpoint\")\n",
        "        print(boxes)\n",
        "        plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), boxes)"
      ],
      "metadata": {
        "id": "Andortt4RbYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "        test_loader,\n",
        "        model,\n",
        "        iou_threshold=NMS_IOU_THRESH,\n",
        "        anchors=ANCHORS,\n",
        "        threshold=0.05,\n",
        "    )\n",
        "\n",
        "print(len(pred_boxes))\n",
        "mapval = mean_average_precision(\n",
        "        pred_boxes,\n",
        "        true_boxes,\n",
        "        iou_threshold=MAP_IOU_THRESH,\n",
        "        box_format=\"midpoint\",\n",
        "        num_classes=NUM_CLASSES,\n",
        "    )\n",
        "print(f\"MAP: {mapval.item()}\")"
      ],
      "metadata": {
        "id": "1_oh3IMYkh0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "f2v23nFRxzdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /content/boards"
      ],
      "metadata": {
        "id": "oNJePfqxx0HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"/content/drive/MyDrive/Models/detection_YOLO_mAP=0,9449\")"
      ],
      "metadata": {
        "id": "O6u4cyvISdNG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}